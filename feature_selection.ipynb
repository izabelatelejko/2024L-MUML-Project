{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import mutual_info_score as MI\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SequentialFeatureSelector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "        n=1000, \n",
    "        n_rel=5, \n",
    "        n_irrel=30, \n",
    "        betas=[1, 2, 3, 4, 5, 6], \n",
    "        n_classes=10,\n",
    "        dataset_variant=0,\n",
    "    ):\n",
    "    \"\"\"Method generates synthetic data and target variable. \n",
    "\n",
    "    X1 are relevant features - first n_rel columns in X\n",
    "    X2, X3, X4, and X5 are irrelevant features.\n",
    "\n",
    "    Args:\n",
    "        n: number of rows to generate\n",
    "        n_rel: number of relevant features that will be generated from normal distribution\n",
    "        n_iirel: number of irrelevant features that will be generated from normal distribution\n",
    "        n_const: number of features with constant values\n",
    "        betas: list of coefficients for calculating target variable (must be of len n_rel + 1)\n",
    "            the first element is bias\n",
    "        n_classes: number of classes in target variable, classes are distributed evenly\n",
    "        dataset_variant: defines which type of irrelevant features will be included in dataset:\n",
    "                        - 0: include irrelevant features (y is not created based on them),\n",
    "                        - 1: include copy of relevant features with added gaussian noise,\n",
    "                        - 2: include interactions between relevant variables.\n",
    "\n",
    "    Returns:\n",
    "        X: synthetic data\n",
    "        y: target variable\n",
    "    \"\"\"\n",
    "    assert len(betas) == (n_rel + 1), 'len of betas must be equal to (n_rel + 1)'\n",
    "\n",
    "    # relevant features from normal distribution\n",
    "    X1 = np.random.normal(0, 1, (n, n_rel))\n",
    "\n",
    "    # target variable\n",
    "    y = X1 @ np.array(betas[1:]).T + betas[0]\n",
    "    y = pd.qcut(y, n_classes, labels=False) \n",
    "\n",
    "    if dataset_variant == 0:\n",
    "        # irrelevant features from normal distribution\n",
    "        X2 = np.random.normal(0, 1, (n, n_irrel))\n",
    "        \n",
    "        X = np.concatenate([X1, X2], axis=1)\n",
    "    elif dataset_variant == 1:\n",
    "        # relevant features with noise\n",
    "        X3 = X1 + np.random.normal(0, 0.1, (n, n_rel))\n",
    "\n",
    "        X = np.concatenate([X1, X3], axis=1)\n",
    "    else:\n",
    "        # second order interactions of relevant features\n",
    "        X4 = np.empty((n,0), float)\n",
    "        for i in range(n_rel - 1):\n",
    "            for j in range(i + 1, n_rel):\n",
    "                X4 = np.append(X4, np.expand_dims(X1[:, i] * X1[:, j], 1), axis=1)\n",
    "\n",
    "        X = np.concatenate([X1, X4], axis=1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CMI(X, Y, Z):\n",
    "  cmi = 0\n",
    "  for z in np.unique(Z):\n",
    "    cmi += MI(X[Z == z], Y[Z == z]) * (len(Z[Z == z]) / len(Z))\n",
    "  return cmi\n",
    "\n",
    "def interaction_gain(X1, X2, Y):\n",
    "   joint_mi = MI(X2, Y) + CMI(X1, Y, X2)\n",
    "   single_mis = MI(X1, Y) + MI(X2, Y)\n",
    "   return joint_mi - single_mis\n",
    "\n",
    "def CMIM(X, y):\n",
    "  print('Calculating CMIM selection...')\n",
    "  chosen_indices = []\n",
    "  for iter_num in range(5):\n",
    "    j_values = []\n",
    "    for i in range(X.shape[1]):\n",
    "        if i in chosen_indices:\n",
    "            j_values.append(-10000)\n",
    "            continue\n",
    "        J = MI(X[:, i], y)\n",
    "        max_value = -10000\n",
    "        for j in chosen_indices:\n",
    "            curr_value = MI(X[:, i], X[:, j]) - CMI(X[:, i], X[:, j], y)\n",
    "            if curr_value > max_value:\n",
    "                max_value = curr_value\n",
    "        j_values.append(J - max_value)\n",
    "    chosen_indices.append(np.argmax(j_values))\n",
    "  \n",
    "  return chosen_indices\n",
    "\n",
    "def JMIM(X, y):\n",
    "  print('Calculating JMIM selection...')\n",
    "  max_mi = -10000\n",
    "  first_idx = None\n",
    "  for i in range(X.shape[1]):\n",
    "    curr_mi = MI(X[:, i], y)\n",
    "    if curr_mi > max_mi:\n",
    "        first_idx = i\n",
    "        max_mi = curr_mi\n",
    "  \n",
    "  chosen_indices = [first_idx]\n",
    "  for iter_num in range(4):\n",
    "    j_values = []\n",
    "    for i in range(X.shape[1]):\n",
    "      if i in chosen_indices:\n",
    "          j_values.append(-10000)\n",
    "          continue\n",
    "      min_value = 10000\n",
    "      for j in chosen_indices:\n",
    "        curr_value = MI(X[:, j], y) + CMI(X[:, i], y, X[:, j])\n",
    "        if curr_value < min_value:\n",
    "            min_value = curr_value\n",
    "      j_values.append(min_value)\n",
    "    chosen_indices.append(np.argmax(j_values))\n",
    "  \n",
    "  return chosen_indices\n",
    "\n",
    "def IGFS(X, y):\n",
    "  print('Calculating IGFS selection...')\n",
    "  chosen_indices = []\n",
    "  for iter_num in range(5):\n",
    "    j_values = []\n",
    "    for i in range(X.shape[1]):\n",
    "      if i in chosen_indices:\n",
    "          j_values.append(-10000)\n",
    "          continue\n",
    "      J = MI(X[:, i], y)\n",
    "      inter_gain_sum = 0\n",
    "      for j in chosen_indices:\n",
    "        inter_gain_sum += interaction_gain(X[:, i], X[:, j], y)\n",
    "      if len(chosen_indices) != 0:\n",
    "        inter_gain_sum /= len(chosen_indices)\n",
    "      j_values.append(J + inter_gain_sum)\n",
    "    chosen_indices.append(np.argmax(j_values))\n",
    "  \n",
    "  return chosen_indices\n",
    "\n",
    "def wrapper_criterion(X, y, criterion=\"bic\"):\n",
    "    print('Calculating BIC selection...')\n",
    "    k = X.shape[1]\n",
    "    included = []\n",
    "    best = None\n",
    "    while True:\n",
    "        value = []\n",
    "        for i in range(k):\n",
    "            if i not in included:\n",
    "                model = sm.OLS(y, sm.add_constant(X[:, included + [i]])).fit()\n",
    "                if criterion == \"bic\":\n",
    "                  score_val = model.bic\n",
    "                elif criterion == \"aic\":\n",
    "                  score_val = model.aic\n",
    "                value.append((score_val, i))\n",
    "        if not value:\n",
    "            break\n",
    "        value.sort()\n",
    "        new_score, new_feature = value[0]\n",
    "        if best is None or new_score < best:\n",
    "            included.append(new_feature)\n",
    "            best = new_score\n",
    "        else:\n",
    "            break\n",
    "    model = sm.OLS(y, sm.add_constant(X[:, included])).fit()\n",
    "    return model, included\n",
    "\n",
    "def l1_selection(X, y):\n",
    "  print(\"Calculating L1 selection...\")\n",
    "  sfs_model = LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"liblinear\").fit(X, y)\n",
    "  # sfs_model = RandomForestClassifier(max_depth=3).fit(X, y)\n",
    "  sfs_forward = SequentialFeatureSelector(\n",
    "      sfs_model, n_features_to_select=\"auto\", tol=0.001, direction=\"forward\"\n",
    "  ).fit(X, y)\n",
    "  return list(np.argwhere(sfs_forward.get_support()*1 > 0).T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_dataset(X, bins=10):\n",
    "    X_discr = np.copy(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        X_discr[:, i] = pd.cut(X[:, i], bins=bins, labels=False)\n",
    "    \n",
    "    return X_discr\n",
    "\n",
    "def find_relevant_features(X, y):\n",
    "    relevant_features = {}\n",
    "    relevant_features[\"BIC\"] = wrapper_criterion(X, y)[1]\n",
    "    relevant_features[\"CMIM\"] = CMIM(X, y)\n",
    "    relevant_features[\"JMIM\"] = JMIM(X, y)\n",
    "    relevant_features[\"IGFS\"] = IGFS(X, y)\n",
    "    relevant_features[\"L1\"] = l1_selection(X, y)\n",
    "\n",
    "    print(\"Calculations completed!\")\n",
    "    return relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# Divorce dataset, source https://www.kaggle.com/datasets/rabieelkharoua/split-or-stay-divorce-predictor-dataset\n",
    "divorce = pd.read_csv(\"data/divorce.csv\", sep=\";\")\n",
    "\n",
    "data[\"divorce\"] = {}\n",
    "data[\"divorce\"][\"X_orig\"] = divorce.drop(\"Class\", axis=1).to_numpy()\n",
    "data[\"divorce\"][\"X_discr\"] = divorce.drop(\"Class\", axis=1).to_numpy()\n",
    "data[\"divorce\"][\"y\"] = divorce[\"Class\"].to_numpy()\n",
    "\n",
    "\n",
    "# AIDS classification dataset, source: https://www.kaggle.com/datasets/aadarshvelu/aids-virus-infection-prediction\n",
    "aids = pd.read_csv(\"data/aids.csv\")\n",
    "X_aids = aids.drop(\"infected\", axis=1).to_numpy()\n",
    "\n",
    "data[\"aids\"] = {}\n",
    "data[\"aids\"][\"X_orig\"] = X_aids\n",
    "data[\"aids\"][\"X_discr\"] = discretize_dataset(X_aids)\n",
    "data[\"aids\"][\"y\"] = aids[\"infected\"].to_numpy()\n",
    "\n",
    "\n",
    "# Heart attack prediction dataset, source: https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n",
    "heart = pd.read_csv(\"data/heart.csv\")\n",
    "X_heart = heart.drop(\"output\", axis=1).to_numpy()\n",
    "\n",
    "data[\"heart\"] = {}\n",
    "data[\"heart\"][\"X_orig\"] = X_heart\n",
    "data[\"heart\"][\"X_discr\"] = discretize_dataset(X_heart)\n",
    "data[\"heart\"][\"y\"] = heart[\"output\"].to_numpy()\n",
    "\n",
    "# LOL Diamond FF15 dataset, source: https://www.kaggle.com/datasets/jakejoeanderson/league-of-legends-diamond-matches-ff15\n",
    "lol = pd.read_csv(\"data/lol.csv\")\n",
    "X_lol = lol.drop([\"match_id\", \"blue_Win\"], axis=1).to_numpy()\n",
    "\n",
    "data[\"lol\"] = {}\n",
    "data[\"lol\"][\"X_orig\"] = X_lol\n",
    "data[\"lol\"][\"X_discr\"] = discretize_dataset(X_lol)\n",
    "data[\"lol\"][\"y\"] = lol[\"blue_Win\"].to_numpy()\n",
    "\n",
    "\n",
    "# Water quality dataset, source: https://www.kaggle.com/datasets/adityakadiwal/water-potability\n",
    "water = pd.read_csv(\"data/water.csv\")\n",
    "water = water.loc[water.ammonia != \"#NUM!\" , :]\n",
    "water.is_safe = water[\"is_safe\"].astype(\"int\")\n",
    "water.ammonia = water[\"ammonia\"].astype(\"float\")\n",
    "X_water = water.drop(\"is_safe\", axis=1).to_numpy()\n",
    "\n",
    "data[\"water\"] = {}\n",
    "data[\"water\"][\"X_orig\"] = X_water\n",
    "data[\"water\"][\"X_discr\"] = discretize_dataset(X_water)\n",
    "data[\"water\"][\"y\"] = water[\"is_safe\"].to_numpy()\n",
    "\n",
    "\n",
    "# Generated data\n",
    "X_gen, y_gen = generate_data(\n",
    "    betas=[6, 5, 4, 3, 2, 1],\n",
    "    dataset_variant=0\n",
    ")\n",
    "\n",
    "data[\"generated\"] = {}\n",
    "data[\"generated\"][\"X_orig\"] = X_gen\n",
    "data[\"generated\"][\"X_discr\"] = discretize_dataset(X_gen)\n",
    "data[\"generated\"][\"y\"] = y_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [key for key, _ in data.items()]\n",
    "relevant_features = {}\n",
    "\n",
    "for key in datasets:\n",
    "    print(f\"Finding relevant features for {key} dataset...\")\n",
    "    relevant_features[key] = find_relevant_features(data[key][\"X_discr\"], data[key][\"y\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
